{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tensorboard in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: scikit-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.16)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.3.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (2.2.2)\n",
      "Collecting nltk (from rouge-score)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (2.39.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m220.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m195.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading lxml-5.3.2-cp310-cp310-manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m199.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m156.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24986 sha256=ad031774275fb3a0dc9266e40da7fe19770fc76eea9b1d2b814079e6c8110359\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: xxhash, tabulate, safetensors, regex, pyarrow, portalocker, lxml, fsspec, dill, colorama, sacrebleu, nltk, multiprocess, huggingface-hub, tokenizers, rouge-score, transformers, accelerate, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "Successfully installed accelerate-1.6.0 colorama-0.4.6 datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 huggingface-hub-0.30.2 lxml-5.3.2 multiprocess-0.70.16 nltk-3.9.1 portalocker-3.1.1 pyarrow-19.0.1 regex-2024.11.6 rouge-score-0.1.2 sacrebleu-2.5.1 safetensors-0.5.3 tabulate-0.9.0 tokenizers-0.21.1 transformers-4.51.3 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets evaluate sacrebleu rouge-score numpy accelerate tensorboard scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer: google/flan-t5-base\n",
      "Enabling gradient checkpointing...\n",
      "Loading dataset from: /teamspace/studios/this_studio/dataset/train.jsonl\n",
      "Reading data from /teamspace/studios/this_studio/dataset/train.jsonl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 17350 examples\n",
      "Dataset size: 17350 examples\n",
      "Sample example source: A small home with 4 rooms on single floor, measuring 14x14 units.\n",
      "\n",
      "A kitchen makes up room 1 containing a counter and a single dishwasher.\n",
      "Room 2, a living room, is furnished with a end table, one lam...\n",
      "Sample example target: {\"id\": \"house_1\", \"numRooms\": 4, \"floors\": 1, \"dimensions\": {\"x\": 14, \"y\": 14}, \"rooms\": [{\"roomType\": \"kitchen\", \"name\": \"kitchen\", \"floorLevel\": 0, \"objects\": [{\"objectType\": \"counter\", \"assetId\": \"...\n",
      "Splitting dataset...\n",
      "Train size: 13880\n",
      "Validation size: 1735\n",
      "Test size: 1735\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5505e15367465badf1e68728d06dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3626ab82d94f4ee59001fbf432b18118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970acb0402584d8d9b24d28e3899f34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tokenized example structure:\n",
      "Keys: ['input_ids', 'attention_mask', 'labels']\n",
      "Input_ids shape: 256\n",
      "Labels shape: 512\n",
      "Valid label tokens: 512 out of 512\n",
      "Max token ID: 31987, Vocab size: 32100\n",
      "Initializing trainer...\n",
      "Starting training with 5 epochs...\n",
      "Debug input batch shapes:\n",
      "input_ids: torch.Size([4, 256]), dtype: torch.int64\n",
      "attention_mask: torch.Size([4, 256]), dtype: torch.int64\n",
      "labels: torch.Size([4, 512]), dtype: torch.int64\n",
      "decoder_input_ids: torch.Size([4, 512]), dtype: torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug input batch shapes:\n",
      "input_ids: torch.Size([4, 256]), dtype: torch.int64\n",
      "attention_mask: torch.Size([4, 256]), dtype: torch.int64\n",
      "labels: torch.Size([4, 512]), dtype: torch.int64\n",
      "decoder_input_ids: torch.Size([4, 512]), dtype: torch.int64\n",
      "Debug input batch shapes:\n",
      "input_ids: torch.Size([4, 256]), dtype: torch.int64\n",
      "attention_mask: torch.Size([4, 256]), dtype: torch.int64\n",
      "labels: torch.Size([4, 512]), dtype: torch.int64\n",
      "decoder_input_ids: torch.Size([4, 512]), dtype: torch.int64\n",
      "Debug input batch shapes:\n",
      "input_ids: torch.Size([4, 256]), dtype: torch.int64\n",
      "attention_mask: torch.Size([4, 256]), dtype: torch.int64\n",
      "labels: torch.Size([4, 512]), dtype: torch.int64\n",
      "decoder_input_ids: torch.Size([4, 512]), dtype: torch.int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2610' max='4335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2610/4335 4:30:27 < 2:58:53, 0.16 it/s, Epoch 3.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>0.254159</td>\n",
       "      <td>81.951200</td>\n",
       "      <td>0.713382</td>\n",
       "      <td>0.520219</td>\n",
       "      <td>0.631682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.244886</td>\n",
       "      <td>82.624378</td>\n",
       "      <td>0.724419</td>\n",
       "      <td>0.531773</td>\n",
       "      <td>0.637353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.240150</td>\n",
       "      <td>82.559500</td>\n",
       "      <td>0.717977</td>\n",
       "      <td>0.526450</td>\n",
       "      <td>0.640531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing evaluation metrics...\n",
      "Predictions - type: <class 'numpy.ndarray'>, shape: (1735, 512), dtype: int64\n",
      "Labels - type: <class 'numpy.ndarray'>, shape: (1735, 512), dtype: int64\n",
      "Predictions (as np array) - shape: (1735, 512), dtype: int64\n",
      "Labels (as np array) - shape: (1735, 512), dtype: int64\n",
      "Raw Predictions - min: 0, max: 31987\n",
      "Raw Labels - min: 1, max: 31987\n",
      "Tokenizer pad_token_id: 0 (type: <class 'int'>)\n",
      "Modified Labels - min: 1, max: 31987, dtype: int64\n",
      "Cleaned Predictions - min: 0, max: 31987, dtype: int64\n",
      "Decoding predictions...\n",
      "Decoding labels...\n",
      "Decoded examples (first 3):\n",
      "\n",
      "Example 0:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 7, \"floors\": 1, \"dimensions\": \"x\": 17, \"y\": 17, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_4700\", \"numRooms\": 7, \"floors\": 1, \"dimensions\": \"x\": 17, \"y\": 17, \"rooms\": [\"roomType\"...\n",
      "\n",
      "Example 1:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 8, \"floors\": 1, \"dimensions\": \"x\": 21, \"y\": 21, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_2495\", \"numRooms\": 8, \"floors\": 1, \"dimensions\": \"x\": 18, \"y\": 18, \"rooms\": [\"roomType\"...\n",
      "\n",
      "Example 2:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 6, \"floors\": 1, \"dimensions\": \"x\": 16, \"y\": 16, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_1906\", \"numRooms\": 6, \"floors\": 1, \"dimensions\": \"x\": 16, \"y\": 16, \"rooms\": [\"roomType\"...\n",
      "Computing BLEU...\n",
      "BLEU score: 81.95119992671297\n",
      "Computing ROUGE...\n",
      "ROUGE raw result: {'rouge1': 0.7133818656348374, 'rouge2': 0.5202194640506976, 'rougeL': 0.6316823027184721, 'rougeLsum': 0.6316807355442983}\n",
      "rouge1: 0.7133818656348374\n",
      "rouge2: 0.5202194640506976\n",
      "rougeL: 0.6316823027184721\n",
      "Computed metrics: {'bleu': 81.95119992671297, 'rouge1': 0.7133818656348374, 'rouge2': 0.5202194640506976, 'rougeL': 0.6316823027184721}\n",
      "\n",
      "Computing evaluation metrics...\n",
      "Predictions - type: <class 'numpy.ndarray'>, shape: (1735, 512), dtype: int64\n",
      "Labels - type: <class 'numpy.ndarray'>, shape: (1735, 512), dtype: int64\n",
      "Predictions (as np array) - shape: (1735, 512), dtype: int64\n",
      "Labels (as np array) - shape: (1735, 512), dtype: int64\n",
      "Raw Predictions - min: 0, max: 31987\n",
      "Raw Labels - min: 1, max: 31987\n",
      "Tokenizer pad_token_id: 0 (type: <class 'int'>)\n",
      "Modified Labels - min: 1, max: 31987, dtype: int64\n",
      "Cleaned Predictions - min: 0, max: 31987, dtype: int64\n",
      "Decoding predictions...\n",
      "Decoding labels...\n",
      "Decoded examples (first 3):\n",
      "\n",
      "Example 0:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 7, \"floors\": 1, \"dimensions\": \"x\": 17, \"y\": 17, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_4700\", \"numRooms\": 7, \"floors\": 1, \"dimensions\": \"x\": 17, \"y\": 17, \"rooms\": [\"roomType\"...\n",
      "\n",
      "Example 1:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 8, \"floors\": 1, \"dimensions\": \"x\": 17, \"y\": 17, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_2495\", \"numRooms\": 8, \"floors\": 1, \"dimensions\": \"x\": 18, \"y\": 18, \"rooms\": [\"roomType\"...\n",
      "\n",
      "Example 2:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 6, \"floors\": 1, \"dimensions\": \"x\": 16, \"y\": 16, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_1906\", \"numRooms\": 6, \"floors\": 1, \"dimensions\": \"x\": 16, \"y\": 16, \"rooms\": [\"roomType\"...\n",
      "Computing BLEU...\n",
      "BLEU score: 82.62437770014854\n",
      "Computing ROUGE...\n",
      "ROUGE raw result: {'rouge1': 0.7244187673865989, 'rouge2': 0.53177309790395, 'rougeL': 0.6373529604866957, 'rougeLsum': 0.6372768410166336}\n",
      "rouge1: 0.7244187673865989\n",
      "rouge2: 0.53177309790395\n",
      "rougeL: 0.6373529604866957\n",
      "Computed metrics: {'bleu': 82.62437770014854, 'rouge1': 0.7244187673865989, 'rouge2': 0.53177309790395, 'rougeL': 0.6373529604866957}\n",
      "\n",
      "Computing evaluation metrics...\n",
      "Predictions - type: <class 'numpy.ndarray'>, shape: (1735, 512), dtype: int64\n",
      "Labels - type: <class 'numpy.ndarray'>, shape: (1735, 512), dtype: int64\n",
      "Predictions (as np array) - shape: (1735, 512), dtype: int64\n",
      "Labels (as np array) - shape: (1735, 512), dtype: int64\n",
      "Raw Predictions - min: 0, max: 31987\n",
      "Raw Labels - min: 1, max: 31987\n",
      "Tokenizer pad_token_id: 0 (type: <class 'int'>)\n",
      "Modified Labels - min: 1, max: 31987, dtype: int64\n",
      "Cleaned Predictions - min: 0, max: 31987, dtype: int64\n",
      "Decoding predictions...\n",
      "Decoding labels...\n",
      "Decoded examples (first 3):\n",
      "\n",
      "Example 0:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 7, \"floors\": 1, \"dimensions\": \"x\": 17, \"y\": 17, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_4700\", \"numRooms\": 7, \"floors\": 1, \"dimensions\": \"x\": 17, \"y\": 17, \"rooms\": [\"roomType\"...\n",
      "\n",
      "Example 1:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 6, \"floors\": 1, \"dimensions\": \"x\": 16, \"y\": 16, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_2495\", \"numRooms\": 8, \"floors\": 1, \"dimensions\": \"x\": 18, \"y\": 18, \"rooms\": [\"roomType\"...\n",
      "\n",
      "Example 2:\n",
      "Prediction: \"id\": \"house_2448\", \"numRooms\": 6, \"floors\": 1, \"dimensions\": \"x\": 16, \"y\": 16, \"rooms\": [\"roomType\"...\n",
      "Reference: \"id\": \"house_1906\", \"numRooms\": 6, \"floors\": 1, \"dimensions\": \"x\": 16, \"y\": 16, \"rooms\": [\"roomType\"...\n",
      "Computing BLEU...\n",
      "BLEU score: 82.55950048776978\n",
      "Computing ROUGE...\n",
      "ROUGE raw result: {'rouge1': 0.7179766491358448, 'rouge2': 0.5264496425452013, 'rougeL': 0.6405314321974998, 'rougeLsum': 0.6403746445919438}\n",
      "rouge1: 0.7179766491358448\n",
      "rouge2: 0.5264496425452013\n",
      "rougeL: 0.6405314321974998\n",
      "Computed metrics: {'bleu': 82.55950048776978, 'rouge1': 0.7179766491358448, 'rouge2': 0.5264496425452013, 'rougeL': 0.6405314321974998}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 446\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    444\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m--> 446\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed. Training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_result\u001b[38;5;241m.\u001b[39mtraining_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Print training metrics\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "Cell \u001b[0;32mIn[1], line 401\u001b[0m, in \u001b[0;36mSafeTrainer.training_step\u001b[0;34m(self, model, inputs, optimizer_idx)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# Call the parent training step\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in training step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/accelerate/accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2454\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "BATCH_SIZE = 4  # Reduced batch size to prevent memory issues\n",
    "MAX_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 512\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "OUTPUT_DIR = \"flan-t5-house-model\"\n",
    "DATASET_PATH = \"/teamspace/studios/this_studio/dataset/train.jsonl\"\n",
    "WARMUP_STEPS = 100\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUTPUT_DIR = f\"{OUTPUT_DIR}-{timestamp}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Disable wandb if not needed\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Modified load dataset function with better error handling\n",
    "def load_dataset_from_jsonl(file_path):\n",
    "    examples = []\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                example = json.loads(line)\n",
    "                if \"nl_description\" not in example or \"house_json\" not in example:\n",
    "                    print(f\"Skipping line {i}: missing required keys\")\n",
    "                    continue\n",
    "\n",
    "                # Process the target as before:\n",
    "                house_json = example[\"house_json\"]\n",
    "                for room in house_json.get(\"rooms\", []):\n",
    "                    for obj in room.get(\"objects\", []):\n",
    "                        if \"position\" in obj:\n",
    "                            pos = obj[\"position\"]\n",
    "                            pos[\"x\"] = round(pos[\"x\"], 2)\n",
    "                            pos[\"y\"] = round(pos[\"y\"], 2)\n",
    "                            pos[\"z\"] = round(pos[\"z\"], 2)\n",
    "\n",
    "                examples.append({\n",
    "                    \"source\": example[\"nl_description\"],\n",
    "                    \"target\": json.dumps(house_json)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Successfully loaded {len(examples)} examples\")\n",
    "    return Dataset.from_list(examples)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "print(f\"Loading model and tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "print(\"Enabling gradient checkpointing...\")\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "# Load and prepare datasets with better error handling\n",
    "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
    "try:\n",
    "    dataset = load_dataset_from_jsonl(DATASET_PATH)\n",
    "\n",
    "    print(f\"Dataset size: {len(dataset)} examples\")\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"Sample example source: {dataset[0]['source'][:200]}...\")\n",
    "        print(f\"Sample example target: {dataset[0]['target'][:200]}...\")\n",
    "    else:\n",
    "        raise ValueError(\"Dataset is empty. Please check your data file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Split dataset (80% train, 10% validation, 10% test)\n",
    "print(\"Splitting dataset...\")\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": test_valid_split[\"train\"],\n",
    "    \"test\": test_valid_split[\"test\"]\n",
    "})\n",
    "\n",
    "print(f\"Train size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation size: {len(dataset_dict['validation'])}\")\n",
    "print(f\"Test size: {len(dataset_dict['test'])}\")\n",
    "\n",
    "# Improved preprocess function with better error handling\n",
    "def preprocess_function(examples):\n",
    "    # For debugging\n",
    "    # for i, (source, target) in enumerate(zip(examples[\"source\"][:3], examples[\"target\"][:3])):\n",
    "    #     print(f\"Example {i} - Input: {source[:50]}...\")\n",
    "    #     print(f\"Example {i} - Target: {target[:50]}...\")\n",
    "\n",
    "    inputs = examples[\"source\"]\n",
    "    targets = examples[\"target\"]\n",
    "\n",
    "    # Process inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Process targets\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Process labels to handle invalid tokens\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"].copy()\n",
    "\n",
    "    # Replace padding token id with -100 so it's ignored in loss calculation\n",
    "    for i in range(len(model_inputs[\"labels\"])):\n",
    "        model_inputs[\"labels\"][i] = [\n",
    "            -100 if token == tokenizer.pad_token_id else token\n",
    "            for token in model_inputs[\"labels\"][i]\n",
    "        ]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing with verbosity for debugging\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names,\n",
    "    batch_size=8  # Process in smaller batches\n",
    ")\n",
    "\n",
    "print(\"\\nSample tokenized example structure:\")\n",
    "if len(tokenized_datasets[\"train\"]) > 0:\n",
    "    print(f\"Keys: {list(tokenized_datasets['train'][0].keys())}\")\n",
    "    print(f\"Input_ids shape: {len(tokenized_datasets['train'][0]['input_ids'])}\")\n",
    "    print(f\"Labels shape: {len(tokenized_datasets['train'][0]['labels'])}\")\n",
    "\n",
    "    # Check for valid tokens in labels\n",
    "    labels = tokenized_datasets['train'][0]['labels']\n",
    "    valid_tokens = [t for t in labels if t != -100]\n",
    "    print(f\"Valid label tokens: {len(valid_tokens)} out of {len(labels)}\")\n",
    "\n",
    "    # Check if any tokens are outside vocabulary range\n",
    "    if valid_tokens:\n",
    "        max_token = max(valid_tokens)\n",
    "        print(f\"Max token ID: {max_token}, Vocab size: {tokenizer.vocab_size}\")\n",
    "        if max_token >= tokenizer.vocab_size:\n",
    "            print(f\"WARNING: Found token ID {max_token} >= vocab size {tokenizer.vocab_size}\")\n",
    "\n",
    "# Define data collator with proper handling of tensors\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "# Improved metric calculation with error handling and verbose output\n",
    "def compute_metrics(eval_pred):\n",
    "    print(\"\\nComputing evaluation metrics...\")\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    print(f\"Predictions - type: {type(predictions)}, shape: {getattr(predictions, 'shape', 'N/A')}, dtype: {getattr(predictions, 'dtype', 'N/A')}\")\n",
    "    print(f\"Labels - type: {type(labels)}, shape: {getattr(labels, 'shape', 'N/A')}, dtype: {getattr(labels, 'dtype', 'N/A')}\")\n",
    "\n",
    "    # --- Debugging: Inspect raw predictions and labels ---\n",
    "    try:\n",
    "        # Ensure they are numpy arrays for inspection\n",
    "        if isinstance(predictions, tuple): # Sometimes predictions might be nested\n",
    "             predictions = predictions[0]\n",
    "        if not isinstance(predictions, np.ndarray):\n",
    "            predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray):\n",
    "            labels = np.array(labels)\n",
    "\n",
    "        print(f\"Predictions (as np array) - shape: {predictions.shape}, dtype: {predictions.dtype}\")\n",
    "        print(f\"Labels (as np array) - shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "\n",
    "        # Check min/max values BEFORE any processing\n",
    "        # Handle potential empty arrays\n",
    "        if predictions.size > 0:\n",
    "            pred_min, pred_max = np.min(predictions), np.max(predictions)\n",
    "            print(f\"Raw Predictions - min: {pred_min}, max: {pred_max}\")\n",
    "            # Check for problematic floats if dtype is float\n",
    "            if np.issubdtype(predictions.dtype, np.floating):\n",
    "                 print(f\"Raw Predictions - contains NaN: {np.isnan(predictions).any()}, contains Inf: {np.isinf(predictions).any()}\")\n",
    "                 # Attempt to convert NaNs/Infs or problematic floats if needed\n",
    "                 # predictions = np.nan_to_num(predictions).astype(np.int64) # Example fix\n",
    "        else:\n",
    "             print(\"Raw Predictions - array is empty\")\n",
    "\n",
    "        if labels.size > 0:\n",
    "            # Check labels *before* replacing -100\n",
    "            label_min_raw, label_max_raw = np.min(labels), np.max(labels)\n",
    "            print(f\"Raw Labels - min: {label_min_raw}, max: {label_max_raw}\")\n",
    "        else:\n",
    "            print(\"Raw Labels - array is empty\")\n",
    "\n",
    "        # Check pad token ID\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        print(f\"Tokenizer pad_token_id: {pad_token_id} (type: {type(pad_token_id)})\")\n",
    "        if pad_token_id is None:\n",
    "            print(\"WARNING: tokenizer.pad_token_id is None!\")\n",
    "            # Handle this case, maybe assign 0 or handle differently?\n",
    "            # pad_token_id = 0 # Example fallback\n",
    "\n",
    "        # Replace -100 in the labels AFTER inspection\n",
    "        labels = np.where(labels != -100, labels, pad_token_id)\n",
    "\n",
    "        # Check labels AFTER replacing -100\n",
    "        if labels.size > 0:\n",
    "             label_min_mod, label_max_mod = np.min(labels), np.max(labels)\n",
    "             print(f\"Modified Labels - min: {label_min_mod}, max: {label_max_mod}, dtype: {labels.dtype}\")\n",
    "        else:\n",
    "             print(\"Modified Labels - array is empty\")\n",
    "\n",
    "        # --- End Debugging ---\n",
    "\n",
    "        # Load metrics\n",
    "        bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "        # --- Explicitly Clean Predictions Before Decoding ---\n",
    "        # Clip predictions to be within the valid vocab range might help\n",
    "        # Ensure predictions are integers\n",
    "        if predictions.size > 0:\n",
    "            # Handle potential floats resulting from model errors? (Unlikely but possible)\n",
    "            if np.issubdtype(predictions.dtype, np.floating):\n",
    "                print(\"WARNING: Predictions have float dtype. Converting NaNs/Infs and casting to int.\")\n",
    "                predictions = np.nan_to_num(predictions) # Convert NaN to 0, Inf to large floats\n",
    "                predictions = predictions.astype(np.int64) # Cast to integer\n",
    "\n",
    "            # Clip to valid token ID range [0, vocab_size - 1]\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "            print(f\"Cleaned Predictions - min: {np.min(predictions)}, max: {np.max(predictions)}, dtype: {predictions.dtype}\")\n",
    "\n",
    "\n",
    "        # Decode predictions\n",
    "        print(\"Decoding predictions...\")\n",
    "        # Add error handling specifically around decoding\n",
    "        try:\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        except Exception as decode_err:\n",
    "             print(f\"ERROR during prediction decoding: {decode_err}\")\n",
    "             print(f\"Problematic predictions (first 10 of first batch): {predictions[0, :10] if predictions.ndim > 1 else predictions[:10]}\")\n",
    "             decoded_preds = [\"DECODING_ERROR\"] * len(predictions) # Fallback\n",
    "\n",
    "        # Decode labels\n",
    "        print(\"Decoding labels...\")\n",
    "        try:\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as decode_err:\n",
    "            print(f\"ERROR during label decoding: {decode_err}\")\n",
    "            print(f\"Problematic labels (first 10 of first batch): {labels[0, :10] if labels.ndim > 1 else labels[:10]}\")\n",
    "            decoded_labels = [\"DECODING_ERROR\"] * len(labels) # Fallback\n",
    "\n",
    "\n",
    "        # Print some examples for debugging\n",
    "        print(\"Decoded examples (first 3):\")\n",
    "        for i in range(min(3, len(decoded_preds))):\n",
    "            print(f\"\\nExample {i}:\")\n",
    "            print(f\"Prediction: {decoded_preds[i][:100]}...\")\n",
    "            print(f\"Reference: {decoded_labels[i][:100]}...\")\n",
    "\n",
    "        # Convert decoded outputs to format expected by metrics\n",
    "        decoded_preds_for_bleu = decoded_preds\n",
    "        decoded_refs_for_bleu = [[label] for label in decoded_labels]\n",
    "\n",
    "        # Compute BLEU score\n",
    "        print(\"Computing BLEU...\")\n",
    "        try:\n",
    "            bleu_result = bleu_metric.compute(\n",
    "                predictions=decoded_preds_for_bleu,\n",
    "                references=decoded_refs_for_bleu\n",
    "            )\n",
    "            bleu_score = bleu_result[\"score\"] if bleu_result and \"score\" in bleu_result else 0.0 # Safely access score\n",
    "            print(f\"BLEU score: {bleu_score}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing BLEU: {e}\")\n",
    "            bleu_score = 0.0 # Use float for consistency\n",
    "\n",
    "        # Compute ROUGE scores\n",
    "        print(\"Computing ROUGE...\")\n",
    "        try:\n",
    "            rouge_result = rouge_metric.compute(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "                use_stemmer=True\n",
    "            )\n",
    "\n",
    "            result = {\"bleu\": bleu_score}\n",
    "            print(f\"ROUGE raw result: {rouge_result}\") # Print the full rouge result for inspection\n",
    "\n",
    "            for rouge_type in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                 if rouge_result and rouge_type in rouge_result:\n",
    "                    # ROUGE often returns floats directly now, or objects with attributes\n",
    "                    rouge_value = rouge_result[rouge_type]\n",
    "                    # Check for AggregateResult object (older versions?) or direct float\n",
    "                    if hasattr(rouge_value, \"mid\") and hasattr(rouge_value.mid, \"fmeasure\"): # Older structure?\n",
    "                        result[rouge_type] = float(rouge_value.mid.fmeasure)\n",
    "                    elif hasattr(rouge_value, \"fmeasure\"): # Another possible structure\n",
    "                        result[rouge_type] = float(rouge_value.fmeasure)\n",
    "                    else: # Assume it's a direct float/int\n",
    "                         try:\n",
    "                             result[rouge_type] = float(rouge_value)\n",
    "                         except (ValueError, TypeError) as convert_err:\n",
    "                             print(f\"Could not convert ROUGE value for {rouge_type}: {rouge_value}, Error: {convert_err}\")\n",
    "                             result[rouge_type] = 0.0\n",
    "\n",
    "                    print(f\"{rouge_type}: {result[rouge_type]}\")\n",
    "                 else:\n",
    "                     print(f\"ROUGE type '{rouge_type}' not found in results.\")\n",
    "                     result[rouge_type] = 0.0\n",
    "\n",
    "            print(f\"Computed metrics: {result}\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing ROUGE: {e}\")\n",
    "            # Ensure default values are floats\n",
    "            return {\n",
    "                \"bleu\": bleu_score, \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"General error in compute_metrics: {e}\")\n",
    "        print(traceback.format_exc()) # Print full traceback\n",
    "        # Ensure default values are floats\n",
    "        return {\"bleu\": 0.0, \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "# Training arguments with reduced complexity\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,        # or explicitly max_grad_norm=1.0\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=10,  # More frequent logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=2,  # Reduced beam search complexity\n",
    "    # Set report_to to None to disable wandb integration\n",
    "    report_to=None,\n",
    "    run_name=f\"flan-t5-house-{timestamp}\"  # Set distinct run_name to avoid warning\n",
    ")\n",
    "\n",
    "# Fixed trainer class with correct method signature\n",
    "class SafeTrainer(Seq2SeqTrainer):\n",
    "    def training_step(self, model, inputs, optimizer_idx=None):\n",
    "        try:\n",
    "            # Print input shapes for debugging\n",
    "            if self.state.global_step == 0:\n",
    "                print(\"Debug input batch shapes:\")\n",
    "                for k, v in inputs.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        print(f\"{k}: {v.shape}, dtype: {v.dtype}\")\n",
    "\n",
    "            # Call the parent training step\n",
    "            return super().training_step(model, inputs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in training step: {e}\")\n",
    "            print(f\"Input batch keys: {list(inputs.keys())}\")\n",
    "\n",
    "            # Return a zero loss tensor to continue training\n",
    "            return torch.tensor(0.0, requires_grad=True, device=model.device)\n",
    "\n",
    "# Initialize trainer - fix the deprecation warning by removing tokenizer parameter\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = SafeTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Save model configuration\n",
    "with open(f\"{OUTPUT_DIR}/training_config.json\", \"w\") as f:\n",
    "    config = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"max_target_length\": MAX_TARGET_LENGTH,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"warmup_steps\": WARMUP_STEPS,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"dataset_path\": DATASET_PATH,\n",
    "        \"train_size\": len(dataset_dict[\"train\"]),\n",
    "        \"validation_size\": len(dataset_dict[\"validation\"]),\n",
    "        \"test_size\": len(dataset_dict[\"test\"]),\n",
    "        \"timestamp\": timestamp\n",
    "    }\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Start training with better error handling\n",
    "print(f\"Starting training with {NUM_EPOCHS} epochs...\")\n",
    "try:\n",
    "    # Force sync GPU before training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    print(f\"Training completed. Training loss: {train_result.training_loss}\")\n",
    "\n",
    "    # Print training metrics\n",
    "    train_metrics = train_result.metrics\n",
    "    print(f\"Training metrics: {json.dumps(train_metrics, indent=2)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Training error occurred: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    print(\"Attempting to save partial training results...\")\n",
    "\n",
    "# Try to save the final model\n",
    "try:\n",
    "    print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Evaluation on test set with better error handling\n",
    "try:\n",
    "    print(\"Evaluating on test set...\")\n",
    "    # Clear CUDA cache before evaluation\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    results = trainer.evaluate(tokenized_datasets[\"test\"], metric_key_prefix=\"test\")\n",
    "    print(\"Final test set evaluation results:\")\n",
    "    print(json.dumps(results, indent=2))\n",
    "\n",
    "    # Save test results\n",
    "    with open(f\"{OUTPUT_DIR}/test_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "# Updated function to generate prediction with better error handling\n",
    "def generate_prediction(input_text):\n",
    "    try:\n",
    "        print(f\"Generating prediction for: {input_text[:100]}...\")\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "        # Move to the right device\n",
    "        device = model.device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Generate with safer parameters\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            num_beams=2,  # Reduced complexity\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        print(f\"Generated output: {decoded_output[:100]}...\")\n",
    "        return decoded_output\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generation: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return f\"Error generating prediction: {str(e)}\"\n",
    "\n",
    "# Test with sample inputs\n",
    "try:\n",
    "    if len(dataset_dict[\"test\"]) > 0:\n",
    "        print(\"\\nRunning generation test with sample inputs...\")\n",
    "        # Test with up to 3 samples\n",
    "        num_samples = min(3, len(dataset_dict[\"test\"]))\n",
    "\n",
    "        test_results = []\n",
    "        for i in range(num_samples):\n",
    "            try:\n",
    "                sample_input = dataset_dict[\"test\"][i][\"source\"]\n",
    "                reference = dataset_dict[\"test\"][i][\"target\"]\n",
    "\n",
    "                # Generate prediction\n",
    "                prediction = generate_prediction(sample_input)\n",
    "\n",
    "                test_results.append({\n",
    "                    \"input\": sample_input,\n",
    "                    \"prediction\": prediction,\n",
    "                    \"reference\": reference\n",
    "                })\n",
    "\n",
    "                print(f\"\\nSample {i+1}:\")\n",
    "                print(f\"Input: {sample_input[:100]}...\")\n",
    "                print(f\"Prediction: {prediction[:100]}...\")\n",
    "                print(f\"Reference: {reference[:100]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "\n",
    "        # Save sample predictions\n",
    "        with open(f\"{OUTPUT_DIR}/sample_predictions.json\", \"w\") as f:\n",
    "            json.dump(test_results, f, indent=2)\n",
    "except Exception as e:\n",
    "    print(f\"Error during sample testing: {e}\")\n",
    "\n",
    "# Create a safer inference script\n",
    "inference_script = f\"\"\"#!/usr/bin/env python\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "MODEL_PATH = \"{OUTPUT_DIR}\"\n",
    "MAX_LENGTH = {MAX_LENGTH}\n",
    "MAX_TARGET_LENGTH = {MAX_TARGET_LENGTH}\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    try:\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "        # Move to GPU if available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "\n",
    "        print(f\"Model loaded successfully (device: {{device}})\")\n",
    "        return tokenizer, model, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {{e}}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def simplify_json(json_obj):\n",
    "    # Simplify floats in position data to reduce complexity\n",
    "    if isinstance(json_obj, dict):\n",
    "        for key, value in json_obj.items():\n",
    "            if key == \"position\" and isinstance(value, dict):\n",
    "                # Round position values\n",
    "                for coord in value:\n",
    "                    if isinstance(value[coord], float):\n",
    "                        value[coord] = round(value[coord], 2)\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                simplify_json(value)\n",
    "    elif isinstance(json_obj, list):\n",
    "        for item in json_obj:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                simplify_json(item)\n",
    "    return json_obj\n",
    "\n",
    "def generate_description(input_json, tokenizer, model, device):\n",
    "    try:\n",
    "        # For house JSON input, parse and simplify\n",
    "        if isinstance(input_json, str):\n",
    "            # Check if the input is a path to a JSON file\n",
    "            if input_json.endswith('.json'):\n",
    "                with open(input_json, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "            else:\n",
    "                # Try to parse as direct JSON\n",
    "                data = json.loads(input_json)\n",
    "        else:\n",
    "            data = input_json\n",
    "\n",
    "        # Simplify the JSON to reduce complexity\n",
    "        data = simplify_json(data)\n",
    "        input_json_str = json.dumps(data)\n",
    "\n",
    "        print(\"Tokenizing input...\")\n",
    "        inputs = tokenizer(input_json_str, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                          max_length=MAX_LENGTH, truncation=True)\n",
    "        inputs = {{k: v.to(device) for k, v in inputs.items()}}\n",
    "\n",
    "        print(\"Generating description...\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                num_beams=2,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        description = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        return f\"Error generating description: {{e}}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer, model, device = load_model_and_tokenizer()\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        # Use command line argument as input\n",
    "        input_text = sys.argv[1]\n",
    "    else:\n",
    "        # Otherwise prompt for input\n",
    "        input_text = input(\"Enter house JSON or path to JSON file: \")\n",
    "\n",
    "    print(\"Processing input...\")\n",
    "    result = generate_description(input_text, tokenizer, model, device)\n",
    "    print(\"\\\\nGenerated description:\")\n",
    "    print(result)\n",
    "\"\"\"\n",
    "\n",
    "# Save inference script\n",
    "try:\n",
    "    with open(f\"{OUTPUT_DIR}/inference.py\", \"w\") as f:\n",
    "        f.write(inference_script)\n",
    "\n",
    "    # Make it executable\n",
    "    os.chmod(f\"{OUTPUT_DIR}/inference.py\", 0o755)\n",
    "    print(f\"Created inference script at {OUTPUT_DIR}/inference.py\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating inference script: {e}\")\n",
    "\n",
    "print(f\"\\nTraining completed! Model and resources saved to {OUTPUT_DIR}\")\n",
    "print(\"To use the model for inference, run:\")\n",
    "print(f\"python {OUTPUT_DIR}/inference.py 'your house JSON data'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
